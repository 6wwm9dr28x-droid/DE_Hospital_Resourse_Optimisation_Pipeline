# DE Hospital Resource Optimisation Pipeline

Airflow-orchestrated reproducible data engineering pipeline for hospital resource optimisation.

This repository contains code, DAGs and helper scripts to ingest, clean, analyse and produce tables/figures for a set of research questions about hospital resource utilisation.

Table of contents
- Quick start
- What this repo contains
- Project structure
- Quick mapping: DAGs / scripts -> steps
- Configuration (.env)
- Running locally with Docker Compose
- Running tests locally
- Where outputs go
- Troubleshooting
- Contributing
- License

Quick start (10-minute demo)
1. Clone the repo:
   - git clone https://github.com/6wwm9dr28x-droid/DE_Hospital_Resource_Optimisation_Pipeline.git
   - cd DE_Hospital_Resource_Optimisation_Pipeline

2. Option A — run with Docker Compose (recommended for reproducibility)
   - Ensure Docker and docker-compose are installed.
   - Start services:
     - docker-compose up --build -d
   - Visit Airflow UI (default): http://localhost:8080 (depends on docker-compose)
   - Trigger DAGs manually from Airflow UI or run the run_step*.py scripts inside the container/host as needed.

3. Option B — run locally with Python
   - python -m venv .venv
   - source .venv/bin/activate   # Windows: .\.venv\Scripts\Activate.ps1
   - python -m pip install --upgrade pip
   - python -m pip install -r requirements.txt
   - (optional) python -m pip install -r requirements-dev.txt
   - Run a pipeline stage:
     - python run_step2_ingest_clean.py
     - python run_step3_rq1.py
     - etc.

What this repo contains (high level)
- docker-compose.yaml — convenience setup to run services (Airflow, DB, etc.).
- dags/ — Airflow DAG definitions (or placeholders) that orchestrate pipeline tasks.
- run_step*.py — scripts that correspond to pipeline steps (ingest, analysis, outputs).
- src/ — core library code used by DAGs and scripts.
- data/ — raw and processed data (not included; place or generate sample data here).
- tables/ and figures/ — output artifacts (CSV, PNG, etc.).
- README.md, LICENSE — docs and license.

Project structure (example)
- .github/                -> CI and automation (Actions, Dependabot)
- dags/                   -> DAG files
- src/                    -> Python modules (pipeline logic)
- run_step2_ingest_clean.py
- run_step3_rq1.py
- run_step4_rq2.py
- run_step5_rq3.py
- run_step6_rq4.py
- run_step7_rq5.py
- requirements.txt
- requirements-dev.txt
- docker-compose.yaml
- data/                   -> data storage (gitignored large files)
- tables/, figures/       -> generated outputs

Quick mapping: DAGs / scripts -> steps
- run_step2_ingest_clean.py
  - Purpose: Ingest data sources (local or remote), run cleaning routines, produce canonical cleaned tables in data/processed or tables/.
- run_step3_rq1.py
  - Purpose: Analyse data to answer Research Question 1; produce tables/figures.
- run_step4_rq2.py
  - Purpose: Analyse data to answer Research Question 2.
- run_step5_rq3.py
  - Purpose: Analyse data to answer Research Question 3.
- run_step6_rq4.py
  - Purpose: Analyse data to answer Research Question 4.
- run_step7_rq5.py
  - Purpose: Analyse data to answer Research Question 5.

(If descriptions above are incomplete, I can expand each to document inputs, outputs and runtime expectations).

Configuration (.env)
- Create a file named `.env` in the repository root or set environment variables in your environment / Docker compose.
- A template is provided as `.env.template` in this repo. Copy it to `.env` and update values.

See `.env.template` for the variables used and example values.

Running locally with Docker Compose
- docker-compose up --build -d
- docker-compose logs -f          # watch logs
- docker-compose exec <service> bash   # get a shell inside a service container
- Open Airflow UI at the address configured in docker-compose (commonly http://localhost:8080).

Running tests locally
- python -m venv .venv
- source .venv/bin/activate
- python -m pip install --upgrade pip
- python -m pip install -r requirements-dev.txt
- pytest -q

Where outputs go
- tables/ — generated CSV/TSV tables used in reports.
- figures/ — generated plots (PNG, SVG).
- Figures and tables are generated by the pipeline code; code that writes these is in run_step* scripts or src/ modules.

Troubleshooting & tips
- CI (GitHub Actions): check the Actions tab if a workflow fails. Logs show which step failed.
- Docker issues: make sure Docker Desktop has sufficient memory and disk space.
- Missing data: If a script fails due to missing input files, place small sample files under `data/` or add a data generator script (I can provide one).
- Permission / secrets: never commit real credentials. Use `.env` and Docker secrets for production.

Contributing
- If you plan to accept contributions, consider adding:
  - CONTRIBUTING.md
  - CODE_OF_CONDUCT.md
  - Issue and PR templates
- Small, focused PRs with tests are easiest to review.

Next suggested tasks (I can do each with you)
- Add .env.template (I included one below).
- Add small sample data or a generator script.
- Add per-DAG docs under `docs/`.
- Improve README with a small architecture diagram (Mermaid) or images.

License
- This repository is licensed under the MIT License — see LICENSE.

Contact / author
- Repository owner: 6wwm9dr28x-droid
- Author listed in commits: Makhsudov Magomed

If you'd like, I can now:
- Commit this updated README.md and the `.env.template` for you, or
- Show step-by-step how to add them through the GitHub web UI.
